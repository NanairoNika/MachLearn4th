{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Shchennikov_assignment_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6k2p1_ZQqAU",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 7\n",
        "\n",
        "Train a Transformer model for Machine Translation from Russian to English.  \n",
        "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
        "Make all source and target text to lower case.  \n",
        "Use following tokenization for english:  \n",
        "```\n",
        "import sentencepiece as spm\n",
        "\n",
        "...\n",
        "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "...\n",
        "TGT.build_vocab(..., min_freq=5)\n",
        "...\n",
        "\n",
        "```\n",
        "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
        "Use last 1000 sentences for model evalutation (test dataset).  \n",
        "Use your target sequence tokenization for BLEU score.  \n",
        "Use max_len=50 for sequence prediction.  \n",
        "\n",
        "\n",
        "Hint: You may consider much smaller model, than shown in the example.  \n",
        "\n",
        "Baselines:  \n",
        "[4 point] BLEU = 0.05  \n",
        "[6 point] BLEU = 0.10  \n",
        "[9 point] BLEU = 0.15  \n",
        "\n",
        "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "\n",
        "\n",
        "Readings:\n",
        "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfeGxmYfQqAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from torchtext import datasets, data\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih3KpqkLQqAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize english \n",
        "with open('data/news-commentary-v13.ru-en.en') as f:\n",
        "    with open('data/text.en', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "S067tcEbQqAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize russian\n",
        "with open('data/news-commentary-v13.ru-en.ru') as f:\n",
        "    with open('data/text.ru', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=data/text.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mJkBjJIQqAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load('bpe_ru.model')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "SRC = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', SRC), ('tgt', TGT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6zenhoMQqAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/text.ru') as f:\n",
        "    src_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open('data/text.en') as f:\n",
        "    tgt_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt, tgt_snt))]\n",
        "test = data.Dataset(examples[-1000:], fields)\n",
        "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhxZUlkhQqAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('src: ' + \" \".join(train.examples[100].src))\n",
        "print('tgt: ' + \" \".join(train.examples[100].tgt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dDF4jm1QqAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train), len(valid), len(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaPXc31XQqA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT.build_vocab(train, min_freq=5)\n",
        "SRC.build_vocab(train, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcA5ZQvuQqA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformer import make_model, Batch\n",
        "\n",
        "    \n",
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized = False\n",
        "\n",
        "    def __init__(self, iterator: data.Iterator):\n",
        "#         super(BucketIteratorWrapper,self).__init__()\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = 1\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(\n",
        "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
        "            self.batch_sampler.__iter__()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "    \n",
        "class MyCriterion(nn.Module):\n",
        "    def __init__(self, pad_idx):\n",
        "        super(MyCriterion, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        x = x.contiguous().permute(0,2,1)\n",
        "        ntokens = (target != self.pad_idx).data.sum()\n",
        "        \n",
        "        return self.criterion(x, target) / ntokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IswXgH4W1i_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_idx = SRC.vocab.stoi[\"<pad>\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTdT-ZkmQqA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 4\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), \n",
        "                                              batch_sizes=(batch_size, batch_size, batch_size), \n",
        "                                  sort_key=lambda x: len(x.src),\n",
        "                                  shuffle=True,\n",
        "                                  device=DEVICE,\n",
        "                                  sort_within_batch=False)\n",
        "                                  \n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        "test_iter = BucketIteratorWrapper(test_iter)\n",
        "\n",
        "\n",
        "model = make_model(len(TGT.vocab), len(SRC.vocab))\n",
        "model = model.to(DEVICE)\n",
        "criterion = MyCriterion(model.generator)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff1YTzPw8ruC",
        "colab_type": "text"
      },
      "source": [
        "I can't understand what kind of mistake is here in line \"pred = model.forward(batch)\"\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHlycxSvQqBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "        pred = model.forward(batch)\n",
        "        loss = criterion(pred, batch.tgt_y)\n",
        "        loss.backward()\n",
        "        if optimizer:\n",
        "            optimizer.step()\n",
        "        loss_value = loss.detach().item()\n",
        "        total_loss += loss\n",
        "        data_iter.set_postfix(loss = loss)\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "def valid_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        pred = model.forward(batch)\n",
        "        loss = criterion(pred, batch.tgt_y)\n",
        "        loss_value = loss.detach().item()\n",
        "        total_loss += loss\n",
        "        data_iter.set_postfix(loss = loss)\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss = train_epoch(train_iter, model, criterion)\n",
        "    print('train', loss)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = valid_epoch(valid_iter, model, criterion)\n",
        "        scheduler.step(loss)\n",
        "        print('valid', loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUx9aODOQqBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(model, src, src_mask, max_len=10, k=5):\n",
        "    <TODO>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9r7XNK3FQqBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src = batch.src[:1]\n",
        "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "        beam = beam_search(model, src, src_key_padding_mask)\n",
        "        \n",
        "        seq = []\n",
        "        for i in range(1, src.size(1)):\n",
        "            sym = SRC.vocab.itos[src[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_ru.decode_pieces(seq)\n",
        "        print(\"\\nSource:\", seq)\n",
        "        \n",
        "        print(\"Translation:\")\n",
        "        for pred, pred_proba in beam:                \n",
        "            seq = []\n",
        "            for i in range(1, pred.size(1)):\n",
        "                sym = TGT.vocab.itos[pred[0, i]]\n",
        "                if sym == \"</s>\": break\n",
        "                seq.append(sym)\n",
        "            seq = tok_en.decode_pieces(seq)\n",
        "            print(f\"pred {pred_proba:.2f}:\", seq)\n",
        "                \n",
        "        seq = []\n",
        "        for i in range(1, batch.tgt.size(1)):\n",
        "            sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_en.decode_pieces(seq)\n",
        "        print(\"Target:\", seq)\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGo6nRzQQqBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXHHGq3AQqBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        <TODO>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-TviH3ZQqBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_bleu(references, hypotheses, \n",
        "            smoothing_function=translate.bleu_score.SmoothingFunction().method3,\n",
        "            auto_reweigh=True\n",
        "           )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}